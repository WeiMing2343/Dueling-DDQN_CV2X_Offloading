{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DQN_networks import Agent\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from utils import plotLearning\n",
    "from enviroment_offloading import offloading_env\n",
    "import pandas as pd\n",
    "VtoM_trans = 2048  #Mbps以5G傳輸速度為範本\n",
    "VtoV_trans = 3072  #Mbps\n",
    "VtoC_trans = 2048  #Mbps\n",
    "\n",
    "car_process = 300  #Mbps\n",
    "MEC_process = 2000 #Mbps\n",
    "if __name__ =='__main__':\n",
    "    #tf.compat.v1.disable_eager_execution()#關閉tensorflow2裡的eager_execution，eager_execution這會讓tf執行的很慢\n",
    "    \n",
    "    #創建環境\n",
    "    #env = gym.make('LunarLander-v2')\n",
    "    #pig 這一段要換成自己寫的車載環境 \n",
    "    #env = single_env()\n",
    "    env = offloading_env(VtoM_trans,VtoV_trans,VtoC_trans,car_process,MEC_process)\n",
    "    lr = 0.0001 #學習率\n",
    "    n_games = 1000 #epoch\n",
    "    #agent = Agent(gamma=0.99, epsilon=1.0, lr=lr, input_dims=env.observation_space.shape,\n",
    "    #              n_actions = env.action_space.n, mem_size=1000000, batch_size=64, epsilon_end=0.01)\n",
    "    #pig n_actions這個要換成自己從車載環境回傳的動作空間,或是直接給值,ex:直接給5(env.action_space.n 輸出為常數)\n",
    "    #pig env.observation_space.shape 要換成自己觀察向量的維度，但要注意gym裡是列表型態，要設成一個值的話，DQN_networks 裡buffer裡的*要去掉。不然就是env.observation_space.shape要為列表型態\n",
    "    agent = Agent(gamma=0.1, epsilon=1.0, lr=lr, input_dims=env.observation_space,\n",
    "                  n_actions = env.action_space, mem_size=1000000, batch_size=64, epsilon_end=0.01)\n",
    "    \n",
    "    score_history = [] #紀錄得分歷史\n",
    "    eps_history = [] #紀錄epsilon歷史\n",
    "    best_score = -1.6#平均要超過才能保存\n",
    "    for i in range(n_games): #主訓練迴圈,訓練幾圈由epoch決定\n",
    "        \n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        setp_number = -1\n",
    "        car1data = 0\n",
    "        car2data = 0\n",
    "        MECdata = 0\n",
    "        score = 0\n",
    "        \n",
    "        \n",
    "        while not done: #done如果false, not done=not false=true=1 ,while判斷為1就等於是無限迴圈,要終止就要done的值為true,not done才會為false\n",
    "            setp_number =setp_number +1\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info,car1data ,car2data ,MECdata  ,csv_latency = env.step(action,setp_number,car1data ,car2data ,MECdata)            \n",
    "#             print(\"第\",setp_number,\"個action = \", action)\n",
    "#             print(\"延遲 = \",reward)\n",
    "#             print(\"第一台車現在的數據輛 = \",observation_[0])\n",
    "#             print(\"第一台車之前的數據輛 = \",car1data)  \n",
    "            \n",
    "#             print(\"第二台車現在的數據輛 = \",observation_[1])\n",
    "#             print(\"第二台車之前的數據輛 = \",car2data)\n",
    "            \n",
    "#             print(\"MEC現在的數據輛 = \",observation_[2])   \n",
    "#             print(\"MEC之前的數據輛 = \",MECdata)\n",
    "#             print(\"========================================\")\n",
    "            score += reward #這次得到的分數就是這一圈epoch的所有reward加總的值,然後迴圈一直更新,直到done為true,就是跑完這次epoch的分數\n",
    "            #score = score - reward\n",
    "            agent.store_transition(observation, action, reward, observation_, done) #把這個迴圈的資料(觀察,動作,獎勵,下一個觀察,終端狀態)存進replay buffer\n",
    "            observation = observation_ #要在while裡更新新的狀態,不然會一直是舊得observation進行learn\n",
    "            \n",
    "            agent.learn() #迭代中進行TD學習\n",
    "        \n",
    "        eps_history.append(agent.epsilon) #把每一個epoch的epsilon記錄下來\n",
    "        score = score/(env.data_time_slot)\n",
    "        \n",
    "        score_history.append(score) #把每一個epoch的分數記錄下來\n",
    "        \n",
    "        avg_score = np.mean(score_history[-100:]) #計算平均分數,為最新一百場比賽的分數加起來平均\n",
    "        if score > best_score : #如果平均分數大於最佳分數\n",
    "            best_score = score  #覆蓋掉最佳分數\n",
    "            print(\"best_score loading = \",best_score)\n",
    "            best_latency_csv=pd.DataFrame(data=csv_latency)\n",
    "            best_latency_csv.to_csv('best_score_latency.csv',encoding='utf-8', index = False)\n",
    "\n",
    "        print('episode: ', i, 'score %.5f' % score, 'average_score %.5f' % avg_score, 'epsilon %.2f' % agent.epsilon)\n",
    "        \n",
    "        \n",
    "    filename = 'offloading_DQN_1.png' #生成的收斂圖名稱,有調整超參數的話可以改變圖的名稱比較好分辨\n",
    "    figure_file = 'plots/' + filename\n",
    "    x = [i+1 for i in range(n_games)] #x軸,i+1=1\n",
    "    plotLearning(x, score_history, eps_history, figure_file) #畫圖\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "def plotLearning(x, scores, epsilons, filename, lines=None):\n",
    "    for i in range(1000):\n",
    "        scores[i] =-scores[i] \n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111, label=\"1\")\n",
    "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "\n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    ax.set_xlabel(\"Game\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis='x', colors=\"C0\")\n",
    "    ax.tick_params(axis='y', colors=\"C0\")\n",
    "\n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
    "\n",
    "    ax2.plot(x, running_avg, color=\"C1\")\n",
    "    #ax2.xaxis.tick_top()\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    #ax2.set_xlabel('x label 2', color=\"C1\")\n",
    "    ax2.set_ylabel('Score', color=\"C1\")\n",
    "    #ax2.xaxis.set_label_position('top')\n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    #ax2.tick_params(axis='x', colors=\"C1\")\n",
    "    plt.title('DQN_avg_scores')\n",
    "    ymin, ymax = plt.ylim()\n",
    "    ytick_spacing = 0.1\n",
    "    yticks = np.arange(ymin, ymax + ytick_spacing, ytick_spacing)\n",
    "    plt.yticks(yticks)\n",
    "    ax2.tick_params(axis='y', colors=\"C1\")\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            plt.axvline(x=line)\n",
    "            \n",
    "    plt.savefig(filename,bbox_inches='tight')\n",
    "plotLearning(x, score_history, eps_history, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#測試自己的維度\n",
    "import numpy as np\n",
    "from agent import Agent\n",
    "from utils import plot_learning_curve, make_env, manage_memory\n",
    "from gym import wrappers\n",
    "from enviroment_offloading import offloading_env\n",
    "from replay_memory import ReplayBuffer\n",
    "#環境制定\n",
    "VtoM_trans = 2048  #Mbps以5G傳輸速度為範本\n",
    "VtoV_trans = 3072  #Mbps\n",
    "VtoC_trans = 2048  #Mbps\n",
    "\n",
    "car_process = 300  #Mbps\n",
    "MEC_process = 30000 #Mbps\n",
    "\n",
    "#datasize_car = np.random.randint(low = 100,high = 900,size = (500,3))\n",
    "#data_time_slot = 500 #總共產生幾筆數據，秒為單位\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    manage_memory()\n",
    "    #env1 = make_env('PongNoFrameskip-v4')\n",
    "    #dims = env1.observation_space\n",
    "    \n",
    "    env = offloading_env(VtoM_trans,VtoV_trans,VtoC_trans,car_process,MEC_process)\n",
    "    best_score = -np.inf\n",
    "    load_checkpoint = False\n",
    "    record_agent = False\n",
    "    n_games = 200\n",
    "    \n",
    "    \n",
    "    dims = np.zeros([3],dtype = \"float32\")\n",
    "\n",
    "    #dims = np.load('input_dims.npy',allow_pickle=True)\n",
    "    #print(\"input_dims = \",dims)\n",
    "    #print(\"input_dims shape = \",dims.shape)\n",
    "    \n",
    "    \n",
    "    agent = Agent(gamma=0.99, epsilon=1, lr=0.0001,\n",
    "                  input_dims = dims.shape ,n_actions=4, \n",
    "                  mem_size=50000, eps_min=0.1,\n",
    "                  batch_size=32, replace=1000, eps_dec=1e-5,\n",
    "                  chkpt_dir='models/', algo='DQNAgent',\n",
    "                  env_name='enviroment_offloading')\n",
    "    if load_checkpoint:\n",
    "        agent.load_models()\n",
    "        agent.epsilon = agent.eps_min\n",
    "\n",
    "    fname = agent.algo + '_' + agent.env_name + '_lr' + str(agent.lr) + '_' \\\n",
    "        + str(n_games) + 'games'\n",
    "    figure_file = 'plots/' + fname + '.png'\n",
    "    # if you want to record video of your agent playing, do a\n",
    "    # mkdir video\n",
    "    n_steps = 0\n",
    "    scores, eps_history, steps_array = [], [], []\n",
    "\n",
    "    for i in range(n_games):\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        setp_number = 0\n",
    "        car1data = 0\n",
    "        car2data = 0\n",
    "        MECdata = 0\n",
    "        score = 0\n",
    "        \n",
    "        while not done:\n",
    "           \n",
    "            setp_number =setp_number +1\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info,car1data ,car2data ,MECdata  ,total_latency = env.step(action,setp_number,car1data ,car2data ,MECdata)\n",
    "#             print(\"第\",setp_number,\"個action = \", action)\n",
    "#             print(\"延遲 = \",reward)\n",
    "#             print(\"第一台車現在的數據輛 = \",observation_[0])\n",
    "#             print(\"第一台車之前的數據輛 = \",car1data)  \n",
    "            \n",
    "#             print(\"第二台車現在的數據輛 = \",observation_[1])\n",
    "#             print(\"第二台車之前的數據輛 = \",car2data)\n",
    "            \n",
    "#             print(\"MEC現在的數據輛 = \",observation_[2])   \n",
    "#             print(\"MEC之前的數據輛 = \",MECdata)\n",
    "#             print(\"========================================\")\n",
    "            score = score - reward\n",
    "            if not load_checkpoint:\n",
    "                agent.store_transition(observation, action,reward, observation_, done)\n",
    "                agent.learn()\n",
    "            observation = observation_\n",
    "            n_steps += 1\n",
    "        \n",
    "        #source = score/setp_number\n",
    "        #print (setp_number)\n",
    "        #print (score)\n",
    "        score = score/(env.data_time_slot)\n",
    "        print (score)\n",
    "        scores.append(score)\n",
    "        steps_array.append(n_steps)\n",
    "        \n",
    "        avg_score = np.mean(scores[-50:])\n",
    "        print('episode {} score {:.4f} avg score {:.4f} '\n",
    "              'best score {:.4f} epsilon {:.2f} steps {}'.\n",
    "              format(i, score, avg_score, best_score, agent.epsilon,\n",
    "                     n_steps))\n",
    "\n",
    "        if score > best_score:\n",
    "            #if not load_checkpoint:\n",
    "                #agent.save_models()\n",
    "            best_score = score\n",
    "            np.savetxt('latency_time.csv',total_latency, delimiter=\",\")\n",
    "\n",
    "        eps_history.append(agent.epsilon)\n",
    "\n",
    "    x = [i+1 for i in range(len(scores))]\n",
    "    plot_learning_curve(steps_array, scores, eps_history, figure_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
