{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### import gym\n",
    "import numpy as np\n",
    "from dueling_ddqn_tf2 import Agent\n",
    "from utils import plotLearning\n",
    "from enviroment_offloading import offloading_env\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "VtoM_trans = 2048  #Mbps以5G傳輸速度為範本\n",
    "VtoV_trans = 3072  #Mbps\n",
    "VtoC_trans = 2048  #Mbps\n",
    "\n",
    "car_process = 300  #Mbps\n",
    "MEC_process = 3200 #Mbps\n",
    "break_count = 0\n",
    "if __name__ == '__main__':\n",
    "    #tf.compat.v1.disable_eager_execution()#關閉tensorflow2裡的eager_execution，eager_execution這會讓tf執行的很慢\n",
    "    #env = gym.make('LunarLander-v2')\n",
    "    env = offloading_env(VtoM_trans,VtoV_trans,VtoC_trans,car_process,MEC_process)\n",
    "    agent = Agent(lr=0.0001, gamma=0.99, n_actions = env.action_space, epsilon=1.0,\n",
    "                  batch_size=64, input_dims=env.observation_space)\n",
    "    n_games = 1500\n",
    "\n",
    "    ddqn_scores = []\n",
    "    eps_history = []\n",
    "    best_score = -1.6#平均要超過才能保存\n",
    "    print(\"break_count = \",break_count)\n",
    "    for i in range(n_games):\n",
    "\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        setp_number = -1\n",
    "        car1data = 0\n",
    "        car2data = 0\n",
    "        MECdata = 0\n",
    "        score = 0\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            setp_number =setp_number +1\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info,car1data ,car2data ,MECdata  ,csv_latency = env.step(action,setp_number,car1data ,car2data ,MECdata)            \n",
    "#             print(\"第\",setp_number,\"個action = \", action)\n",
    "#             print(\"延遲 = \",reward)\n",
    "#             print(\"第一台車現在的數據輛 = \",observation_[0])\n",
    "#             print(\"第一台車之前的數據輛 = \",car1data)  \n",
    "            \n",
    "#             print(\"第二台車現在的數據輛 = \",observation_[1])\n",
    "#             print(\"第二台車之前的數據輛 = \",car2data)\n",
    "            \n",
    "#             print(\"MEC現在的數據輛 = \",observation_[2])   \n",
    "#             print(\"MEC之前的數據輛 = \",MECdata)\n",
    "#             print(\"========================================\")\n",
    "            score += reward #這次得到的分數就是這一圈epoch的所有reward加總的值,然後迴圈一直更新,直到done為true,就是跑完這次epoch的分數\n",
    "            #score = score - reward\n",
    "#             print(\"score = \",score)\n",
    "            agent.store_transition(observation, action, reward, observation_, done) #把這個迴圈的資料(觀察,動作,獎勵,下一個觀察,終端狀態)存進replay buffer\n",
    "            observation = observation_ #要在while裡更新新的狀態,不然會一直是舊得observation進行learn\n",
    "            \n",
    "            agent.learn() #迭代中進行TD學習\n",
    "        eps_history.append(agent.epsilon)\n",
    "        score = score/(env.data_time_slot)\n",
    "        ddqn_scores.append(score)\n",
    "        avg_score = np.mean(ddqn_scores[-50:])\n",
    "\n",
    "        if score > best_score : #如果平均分數大於最佳分數 \n",
    "            best_score = score #覆蓋掉最佳分數\n",
    "            best_latency_csv=pd.DataFrame(data=csv_latency)\n",
    "            print(\"best_score loading = \",best_score)\n",
    "            best_latency_csv.to_csv('best_score_latency.csv',encoding='utf-8', index = False)\n",
    "            \n",
    "        print('episode: ', i,'score: %.5f' % score,' average score %.5f' % avg_score , 'epsilon %.2f' % agent.epsilon )\n",
    "        if avg_score > -0.620:\n",
    "            break_count += break_count\n",
    "            print(\"break_count = \",break_count)\n",
    "        if break_count == 5 :\n",
    "            break\n",
    "    filename = 'offloading_ddqn1.png'\n",
    "\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    plotLearning(x, ddqn_scores, eps_history, filename)\n",
    "    for i in range(1500):\n",
    "        ddqn_scores[i] =-ddqn_scores[i] \n",
    "        \n",
    "    N = len(ddqn_scores)\n",
    "    running_avg = np.empty(N)\n",
    "    \n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(ddqn_scores[max(0, t-50):(t+1)])\n",
    "    print(running_avg)    \n",
    "    test=pd.DataFrame(data=running_avg)\n",
    "    test.to_csv('avg_score_0330_0447.csv',encoding='utf-8', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#local computing \n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "time_slot = 500\n",
    "epoch = 1500\n",
    "car_process = 753  # Mbps\n",
    "csv_latency = np.zeros([epoch], dtype=\"float32\")\n",
    "\n",
    "latency = np.zeros([time_slot], dtype=\"float32\")  # Move this line outside the loop\n",
    "\n",
    "for i in range(epoch):\n",
    "    mu_cpu, sigma_cpu = car_process, 10\n",
    "    car_cpu = np.random.normal(mu_cpu, sigma_cpu, time_slot)\n",
    "    car_cpu = np.clip(car_cpu, car_process - 50, car_process + 50)\n",
    "    car_cpu = car_cpu.astype(int)\n",
    "\n",
    "    mu, sigma = 750, 100\n",
    "    car1_data = np.random.normal(mu, sigma, time_slot)\n",
    "    car1_data = np.clip(car1_data, 500, 1000)\n",
    "    car1_data = car1_data.astype(int)\n",
    "\n",
    "    before_car1_data = 0\n",
    "\n",
    "    latency_total = 0\n",
    "\n",
    "    for step_number in range(time_slot):  # Change the variable name from setp_number to step_number\n",
    "        waiting_data_number = before_car1_data\n",
    "        latency_process = car1_data[step_number] / car_cpu[step_number]\n",
    "        latency_queue = waiting_data_number / car_cpu[step_number]\n",
    "\n",
    "        latency[step_number] = latency_process + latency_queue\n",
    "        before_car1_data = (latency[step_number] - 1) * car_cpu[step_number]\n",
    "\n",
    "        if before_car1_data < 0:\n",
    "            before_car1_data = 0\n",
    "\n",
    "    for j in range(time_slot):  # Use a different variable name here\n",
    "        latency_total += latency[j]\n",
    "    avg_latency = latency_total / time_slot\n",
    "    #print(avg_latency)\n",
    "    \n",
    "    if avg_latency < 4.5 and avg_latency > 4:\n",
    "        print(avg_latency)\n",
    "        latency_pd = pd.DataFrame(data=latency)\n",
    "        latency_pd.to_csv('503_latency.csv', encoding='utf-8', index=False)\n",
    "        #print(latency)\n",
    "\n",
    "\n",
    "    csv_latency[i] = avg_latency\n",
    "\n",
    "print(csv_latency)\n",
    "\n",
    "csv_latency = pd.DataFrame(data=csv_latency)\n",
    "csv_latency.to_csv('local_latency.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cloud computing\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "time_slot = 500\n",
    "epoch = 1500\n",
    "\n",
    "csv_latency = np.zeros([epoch], dtype=\"float32\")\n",
    "\n",
    "latency = np.zeros([time_slot], dtype=\"float32\")  # Move this line outside the loop\n",
    "\n",
    "for i in range(epoch):\n",
    "    mu_cpu, sigma_cpu = 1024, 100\n",
    "    car_cpu = np.random.normal(mu_cpu, sigma_cpu, time_slot)\n",
    "    car_cpu = np.clip(car_cpu, car_cpu - 50, car_cpu + 50)\n",
    "    car_cpu = car_cpu.astype(int)\n",
    "\n",
    "    mu, sigma = 750, 100\n",
    "    car1_data = np.random.normal(mu, sigma, time_slot)\n",
    "    car1_data = np.clip(car1_data, 500, 1000)\n",
    "    car1_data = car1_data.astype(int)\n",
    "\n",
    "    before_car1_data = 0\n",
    "\n",
    "    latency_total = 0\n",
    "\n",
    "    for step_number in range(time_slot):  # Change the variable name from setp_number to step_number\n",
    "\n",
    "        latency_process = car1_data[step_number] / car_cpu[step_number]\n",
    "        latency[step_number] = latency_process \n",
    "\n",
    "\n",
    "    for j in range(time_slot):  # Use a different variable name here\n",
    "        latency_total += latency[j]\n",
    "    avg_latency = latency_total / time_slot\n",
    "    #print(avg_latency)\n",
    "\n",
    "    latency_pd = pd.DataFrame(data=latency)\n",
    "    latency_pd.to_csv('cloud_latency.csv', encoding='utf-8', index=False)\n",
    "        #print(latency)\n",
    "\n",
    "\n",
    "    csv_latency[i] = avg_latency\n",
    "\n",
    "print(csv_latency)\n",
    "\n",
    "csv_latency = pd.DataFrame(data=csv_latency)\n",
    "csv_latency.to_csv('total_cloud_latency.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####!@#$%錯誤的kstest 算法\n",
    "import numpy as np\n",
    "from scipy.stats import kstest, norm\n",
    "import pandas as pd\n",
    "\n",
    "# 讀取csv檔案\n",
    "data = pd.read_csv('best_score_latency_0409.csv')\n",
    "\n",
    "# 將數據轉換成numpy陣列\n",
    "data_array = np.array(data['0'])\n",
    "\n",
    "# 執行KS檢驗\n",
    "statistic, p_value = kstest(data_array, 'norm')\n",
    "\n",
    "# 計算KS檢驗結果的門檻值\n",
    "alpha = 0.01\n",
    "n = len(data_array)\n",
    "ks_threshold = np.sqrt(-0.5 * np.log(alpha / 2)) / np.sqrt(n)\n",
    "\n",
    "# 判斷KS檢驗結果是否通過\n",
    "if statistic < ks_threshold:\n",
    "    print(\"P-value: \", p_value)\n",
    "    print(f\"KS test statistic D = {statistic}\")\n",
    "    print(\"The data follows a normal distribution\")\n",
    "else:\n",
    "    print(\"P-value: \", p_value)\n",
    "    print(f\"KS test statistic D = {statistic}\")\n",
    "    print(\"The data does not follow a normal distribution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算一般常態分佈的 kstest 資料路徑 /plots/perason_kstest_plot/normal_distribution\n",
    "import numpy as np\n",
    "from scipy.stats import kstest, poisson, norm\n",
    "import pandas as pd\n",
    "\n",
    "# 讀取 csv 檔案\n",
    "data = pd.read_csv('plots/perason_kstest_plot/normal_distribution.csv', header=None)\n",
    "#data = pd.read_csv('RE_plot/DDQN_best_latency.csv', header=None)\n",
    "\n",
    "# 轉換為 numpy array\n",
    "sample = np.array(data[0])\n",
    "\n",
    "# 計算平均值\n",
    "sample_mean = np.mean(sample)\n",
    "loc, scale = norm.fit(sample)\n",
    "n = norm(loc=loc, scale=scale)\n",
    "\n",
    "# 計算 KS test\n",
    "#D, p_value = kstest(sample, norm(sample_mean).cdf)\n",
    "D, p_value = kstest(sample, n.cdf)\n",
    "\n",
    "# 列印結果\n",
    "print('KS test statistic D:', D)\n",
    "print('p-value:', p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算DDQN kstest 資料路徑 /plots/perason_kstest_plot/poisson_plot_latency\n",
    "import numpy as np\n",
    "from scipy.stats import kstest, poisson, norm\n",
    "import pandas as pd\n",
    "\n",
    "# 讀取 csv 檔案\n",
    "#data = pd.read_csv('0.csv', header=None)\n",
    "data = pd.read_csv('plots/perason_kstest_plot/poisson_plot_latency.csv', header=None)\n",
    "\n",
    "# 轉換為 numpy array\n",
    "sample = np.array(data[0])\n",
    "\n",
    "# 計算平均值\n",
    "sample_mean = np.mean(sample)\n",
    "loc, scale = norm.fit(sample)\n",
    "n = norm(loc=loc, scale=scale)\n",
    "\n",
    "# 計算 KS test\n",
    "#D, p_value = kstest(sample, norm(sample_mean).cdf)\n",
    "D, p_value = kstest(sample, n.cdf)\n",
    "\n",
    "# 列印結果\n",
    "print('KS test statistic D:', D)\n",
    "print('p-value:', p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#python 版本的Cullen and Frey graph圖*********\n",
    "from scipy.stats import probplot\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 讀取 csv 檔案\n",
    "df = pd.read_csv('plots/perason_kstest_plot/poisson_plot_latency0.csv')\n",
    "\n",
    "# 使用 probplot 函數繪製 Cullen and Frey graph\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "probplot(df['0'], plot=ax, dist='norm', fit=True)\n",
    "ax.set_title('Pearson Plot for Normal Distribution')\n",
    "filename = 'python_Cullen_and_Frey_Graph.png'\n",
    "figure_file = 'plots/perason_kstest_plot/' + filename\n",
    "plt.savefig(figure_file, dpi=1200,bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成常態分佈數據比對DDQN的數據\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import kurtosis, skew\n",
    "import pandas as pd\n",
    "\n",
    "data = np.random.normal(0.6, 0.08, size=1492)\n",
    "\n",
    "best_latency_csv=pd.DataFrame(data=data)\n",
    "best_latency_csv.to_csv('plots/perason_kstest_plot/normal_distribution.csv',encoding='utf-8', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDQN值方圖\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 生成一組隨機數據\n",
    "data = pd.read_csv('plots/perason_kstest_plot/poisson_plot_latency0.csv')#在KStest資料夾\n",
    "data = np.array(data['0'])\n",
    "# 進行正態分布擬合\n",
    "mu, std = norm.fit(data)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.set_title('DDQN Value Square Chart')\n",
    "# 繪製樣本數據的直方圖\n",
    "plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n",
    "\n",
    "# 繪製正態分布的概率密度函數\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "filename = 'Value_Square_Chart.png'\n",
    "figure_file = 'plots/perason_kstest_plot/' + filename\n",
    "plt.savefig(figure_file, dpi=1200,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 輸出擬合後的均值和標準差\n",
    "print(\"Fitted mean = {:.2f}\".format(mu))\n",
    "print(\"Fitted standard deviation = {:.2f}\".format(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#常態分布值方圖\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 生成一組隨機數據\n",
    "data = np.random.randn(1492)\n",
    "\n",
    "# 進行正態分布擬合\n",
    "mu, std = norm.fit(data)\n",
    "\n",
    "# 繪製樣本數據的直方圖\n",
    "plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n",
    "\n",
    "# 繪製正態分布的概率密度函數\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 輸出擬合後的均值和標準差\n",
    "print(\"Fitted mean = {:.2f}\".format(mu))\n",
    "print(\"Fitted standard deviation = {:.2f}\".format(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break_count =  0\n",
      "action_number =  192 103 93 112\n",
      "best_score loading =  -2.990204494348663\n",
      "episode:  0 score: -2.99020  average score -2.99020 epsilon 0.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10656\\3746348175.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_\u001b[0m \u001b[1;31m#要在while裡更新新的狀態,不然會一直是舊得observation進行learn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#迭代中進行TD學習\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m  \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0maction0_number\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\weiming\\Desktop\\Xu_thesis\\Code\\Dueling_DDQN_offloading\\dueling_ddqn_tf2.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[1;31m#q_next[idx] = 0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mq_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mq_next\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_actions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\weiming\\Anaconda3\\envs\\py37tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m    800\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m         \u001b[0mvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 802\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\weiming\\Anaconda3\\envs\\py37tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m    966\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[0;32m    969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\weiming\\Anaconda3\\envs\\py37tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m  10352\u001b[0m         \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"begin_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"end_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10353\u001b[0m         \u001b[1;34m\"ellipsis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"new_axis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10354\u001b[1;33m         \"shrink_axis_mask\", shrink_axis_mask)\n\u001b[0m\u001b[0;32m  10355\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10356\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### 802.11P傳輸DDQN\n",
    "import numpy as np\n",
    "from dueling_ddqn_tf2 import Agent\n",
    "from dueling_ddqn_tf2 import DuelingDeepQNetwork\n",
    "from utils import plotLearning\n",
    "from enviroment_offloading import offloading_env\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "VtoM_trans = 20#110  #Mbps以5G傳輸速度為範本\n",
    "VtoV_trans = 20#165  #Mbps\n",
    "VtoC_trans = 20#110  #Mbps\n",
    "\n",
    "car_process = 40  #Mbps\n",
    "MEC_process = 170 #Mbps\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #tf.compat.v1.disable_eager_execution()#關閉tensorflow2裡的eager_execution，eager_execution這會讓tf執行的很慢\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "    #tf.enable_eager_execution()\n",
    "    #env = gym.make('LunarLander-v2')\n",
    "    env = offloading_env(VtoM_trans,VtoV_trans,VtoC_trans,car_process,MEC_process)\n",
    "    agent = Agent(lr=0.0001, gamma=0.99, n_actions = env.action_space, epsilon=1.0,\n",
    "                  batch_size=64, input_dims=env.observation_space)\n",
    "    \n",
    "    n_games = 1500\n",
    "    break_count = 0\n",
    "    ddqn_scores = []\n",
    "    eps_history = []\n",
    "    best_score = -3#平均要超過才能保存\n",
    "    print(\"break_count = \",break_count)\n",
    "    for i in range(n_games):\n",
    "        action0_number = 0\n",
    "        action1_number = 0\n",
    "        action2_number = 0\n",
    "        action3_number = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        setp_number = -1\n",
    "        car1data = 0\n",
    "        car2data = 0\n",
    "        MECdata = 0\n",
    "        score = 0\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            setp_number =setp_number +1\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info,car1data ,car2data ,MECdata  ,csv_latency = env.step(action,setp_number,car1data ,car2data ,MECdata)            \n",
    "#             print(\"第\",setp_number,\"個action = \", action)\n",
    "#             print(\"延遲 = \",reward)\n",
    "#             print(\"第一台車現在的數據輛 = \",observation_[0])\n",
    "#             print(\"第一台車之前的數據輛 = \",car1data)  \n",
    "            \n",
    "#             print(\"第二台車現在的數據輛 = \",observation_[1])\n",
    "#             print(\"第二台車之前的數據輛 = \",car2data)\n",
    "            \n",
    "#             print(\"MEC現在的數據輛 = \",observation_[2])   \n",
    "#             print(\"MEC之前的數據輛 = \",MECdata)\n",
    "#             print(\"========================================\")\n",
    "            score += reward #這次得到的分數就是這一圈epoch的所有reward加總的值,然後迴圈一直更新,直到done為true,就是跑完這次epoch的分數\n",
    "            #score = score - reward\n",
    "#             print(\"score = \",score)\n",
    "            agent.store_transition(observation, action, reward, observation_, done) #把這個迴圈的資料(觀察,動作,獎勵,下一個觀察,終端狀態)存進replay buffer\n",
    "            observation = observation_ #要在while裡更新新的狀態,不然會一直是舊得observation進行learn\n",
    "            \n",
    "            agent.learn() #迭代中進行TD學習\n",
    "            if action  == 0 :\n",
    "                action0_number += 1\n",
    "            elif action  == 1:\n",
    "                action1_number += 1\n",
    "            elif action  == 2:\n",
    "                action2_number += 1\n",
    "            elif action  == 3:\n",
    "                action3_number += 1\n",
    "        print(\"action_number = \", action0_number,action1_number,action2_number,action3_number)\n",
    "        \n",
    "        eps_history.append(agent.epsilon)\n",
    "        score = score/(env.data_time_slot)\n",
    "        ddqn_scores.append(score)\n",
    "        avg_score = np.mean(ddqn_scores[-50:])\n",
    "        \n",
    "\n",
    "                \n",
    "        \n",
    "        \n",
    "        if score > best_score : #如果平均分數大於最佳分數\n",
    "            best_score = score #覆蓋掉最佳分數\n",
    "            best_latency_csv=pd.DataFrame(data=csv_latency)\n",
    "            print(\"best_score loading = \",best_score)\n",
    "            best_latency_csv.to_csv('80211P_trans_latency.csv',encoding='utf-8', index = False)\n",
    "            \n",
    "        print('episode: ', i,'score: %.5f' % score,' average score %.5f' % avg_score , 'epsilon %.2f' % agent.epsilon )\n",
    "        if avg_score > -0.62:\n",
    "            break_count = break_count + 1\n",
    "            print(\"break_count = \",break_count)\n",
    "        if break_count > 200 and avg_score < -0.62 and i>1000:\n",
    "            n_games = i+1\n",
    "            print(\"n_games = \",n_games)\n",
    "            break\n",
    "    filename = 'offloading_ddqn_0409.png'\n",
    "\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    plotLearning(x, ddqn_scores, eps_history, filename)\n",
    "    for i in range(n_games):\n",
    "        ddqn_scores[i] =-ddqn_scores[i] \n",
    "        \n",
    "    N = len(ddqn_scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(ddqn_scores[max(0, t-50):(t+1)])\n",
    "    print(running_avg)    \n",
    "    test=pd.DataFrame(data=running_avg)\n",
    "    test.to_csv('80211P_trans_avg_score.csv',encoding='utf-8', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 5G 傳輸\n",
    "import numpy as np\n",
    "from dueling_ddqn_tf2 import Agent\n",
    "from utils import plotLearning\n",
    "from enviroment_offloading import offloading_env\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import pydot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "NR_date_rate = 8*8*(948/1024)*(1*12)*(0.001*(14*2**1))*(1-0.14)\n",
    "\n",
    "VtoM_trans = NR_date_rate*7  #110Mbps以5G傳輸速度為範本\n",
    "VtoV_trans = NR_date_rate*10  #165Mbps\n",
    "VtoC_trans = NR_date_rate*7  #110Mbps\n",
    "\n",
    "car_process = 40  #Mbps\n",
    "MEC_process = 165 #Mbps\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    #tf.compat.v1.disable_eager_execution()#關閉tensorflow2裡的eager_execution，eager_execution這會讓tf執行的很慢\n",
    "    #env = gym.make('LunarLander-v2')\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "    env = offloading_env(VtoM_trans,VtoV_trans,VtoC_trans,car_process,MEC_process)\n",
    "    agent = Agent(lr=0.0001, gamma=0.9, n_actions = env.action_space, epsilon=1.0,\n",
    "                  batch_size=64, input_dims=env.observation_space)\n",
    "\n",
    "    \n",
    "    n_games = 2000\n",
    "    break_count = 0\n",
    "    ddqn_scores = []\n",
    "    eps_history = []\n",
    "    best_score = -1.6#平均要超過才能保存\n",
    "    print(\"break_count = \",break_count)\n",
    "    for i in range(n_games):\n",
    "        \n",
    "        action0_number = 0\n",
    "        action1_number = 0\n",
    "        action2_number = 0\n",
    "        action3_number = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        setp_number = -1\n",
    "        car1data = 0\n",
    "        car2data = 0\n",
    "        MECdata = 0\n",
    "        score = 0\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            setp_number =setp_number +1\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info,car1data ,car2data ,MECdata  ,csv_latency = env.step(action,setp_number,car1data ,car2data ,MECdata)            \n",
    "#             print(\"第\",setp_number,\"個action = \", action)\n",
    "#             print(\"延遲 = \",reward)\n",
    "#             print(\"第一台車現在的數據輛 = \",observation_[0])\n",
    "#             print(\"第一台車之前的數據輛 = \",car1data)  \n",
    "            \n",
    "#             print(\"第二台車現在的數據輛 = \",observation_[1])\n",
    "#             print(\"第二台車之前的數據輛 = \",car2data)\n",
    "            \n",
    "#             print(\"MEC現在的數據輛 = \",observation_[2])   \n",
    "#             print(\"MEC之前的數據輛 = \",MECdata)\n",
    "#             print(\"========================================\")\n",
    "            score += reward #這次得到的分數就是這一圈epoch的所有reward加總的值,然後迴圈一直更新,直到done為true,就是跑完這次epoch的分數\n",
    "            #score = score - reward\n",
    "#             print(\"score = \",score)\n",
    "            agent.store_transition(observation, action, reward, observation_, done) #把這個迴圈的資料(觀察,動作,獎勵,下一個觀察,終端狀態)存進replay buffer\n",
    "            observation = observation_ #要在while裡更新新的狀態,不然會一直是舊得observation進行learn\n",
    "            \n",
    "            agent.learn() #迭代中進行TD學習\n",
    "            if action  == 0 :\n",
    "                action0_number += 1\n",
    "            elif action  == 1:\n",
    "                action1_number += 1\n",
    "            elif action  == 2:\n",
    "                action2_number += 1\n",
    "            elif action  == 3:\n",
    "                action3_number += 1\n",
    "        print(\"action_number = \", action0_number,action1_number,action2_number,action3_number)\n",
    "        \n",
    "        \n",
    "        eps_history.append(agent.epsilon)\n",
    "        score = score/(env.data_time_slot)\n",
    "        ddqn_scores.append(score)\n",
    "        avg_score = np.mean(ddqn_scores[-50:])\n",
    "        \n",
    "\n",
    "                \n",
    "        \n",
    "        \n",
    "        if score > best_score : #如果平均分數大於最佳分數\n",
    "            best_score = score #覆蓋掉最佳分數\n",
    "            best_latency_csv=pd.DataFrame(data=csv_latency)\n",
    "            print(\"best_score loading = \",best_score)\n",
    "            best_latency_csv.to_csv('plots/other_data/best_score_latency_0.8.csv',encoding='utf-8', index = False)\n",
    "            \n",
    "        print('episode: ', i,'score: %.5f' % score,' average score %.5f' % avg_score , 'epsilon %.2f' % agent.epsilon )\n",
    "        if i>1000: \n",
    "            if avg_score > -0.62:\n",
    "                break_count = break_count + 1\n",
    "                print(\"break_count = \",break_count)\n",
    "        if break_count > 100 and avg_score > -0.62 :\n",
    "            n_games = i+1\n",
    "            print(\"n_games = \",n_games)\n",
    "            break\n",
    "    filename = 'offloading_ddqn_0.8.png'\n",
    "    end = time.time()\n",
    "    print(\"總共所花的時間\",format(end-start))\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    plotLearning(x, ddqn_scores, eps_history, filename)\n",
    "    for i in range(n_games):\n",
    "        ddqn_scores[i] =-ddqn_scores[i] \n",
    "        \n",
    "    N = len(ddqn_scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(ddqn_scores[max(0, t-50):(t+1)])\n",
    "    print(running_avg)    \n",
    "    test=pd.DataFrame(data=running_avg)\n",
    "    test.to_csv('plots/learning_rate/DDQN_avg_score_0.8.csv',encoding='utf-8', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##learning rate 0.0001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 5G 傳輸\n",
    "import numpy as np\n",
    "from dueling_ddqn_tf2 import Agent\n",
    "from utils import plotLearning\n",
    "from enviroment_offloading import offloading_env\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "VtoM_trans = 110  #Mbps以5G傳輸速度為範本\n",
    "VtoV_trans = 165  #Mbps\n",
    "VtoC_trans = 110  #Mbps\n",
    "\n",
    "car_process = 60  #Mbps\n",
    "MEC_process = 170 #Mbps\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    #tf.compat.v1.disable_eager_execution()#關閉tensorflow2裡的eager_execution，eager_execution這會讓tf執行的很慢\n",
    "    #env = gym.make('LunarLander-v2')\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "    env = offloading_env(VtoM_trans,VtoV_trans,VtoC_trans,car_process,MEC_process)\n",
    "    agent = Agent(lr=0.0001, gamma=0.90, n_actions = env.action_space, epsilon=1.0,\n",
    "                  batch_size=64, input_dims=env.observation_space)\n",
    "    n_games = 1500\n",
    "    break_count = 0\n",
    "    ddqn_scores = []\n",
    "    eps_history = []\n",
    "    best_score = -1.6#平均要超過才能保存\n",
    "    print(\"break_count = \",break_count)\n",
    "    for i in range(n_games):\n",
    "        \n",
    "        action0_number = 0\n",
    "        action1_number = 0\n",
    "        action2_number = 0\n",
    "        action3_number = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        setp_number = -1\n",
    "        car1data = 0\n",
    "        car2data = 0\n",
    "        MECdata = 0\n",
    "        score = 0\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            setp_number =setp_number +1\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info,car1data ,car2data ,MECdata  ,csv_latency = env.step(action,setp_number,car1data ,car2data ,MECdata)            \n",
    "#             print(\"第\",setp_number,\"個action = \", action)\n",
    "#             print(\"延遲 = \",reward)\n",
    "#             print(\"第一台車現在的數據輛 = \",observation_[0])\n",
    "#             print(\"第一台車之前的數據輛 = \",car1data)  \n",
    "            \n",
    "#             print(\"第二台車現在的數據輛 = \",observation_[1])\n",
    "#             print(\"第二台車之前的數據輛 = \",car2data)\n",
    "            \n",
    "#             print(\"MEC現在的數據輛 = \",observation_[2])   \n",
    "#             print(\"MEC之前的數據輛 = \",MECdata)\n",
    "#             print(\"========================================\")\n",
    "            score += reward #這次得到的分數就是這一圈epoch的所有reward加總的值,然後迴圈一直更新,直到done為true,就是跑完這次epoch的分數\n",
    "            #score = score - reward\n",
    "#             print(\"score = \",score)\n",
    "            agent.store_transition(observation, action, reward, observation_, done) #把這個迴圈的資料(觀察,動作,獎勵,下一個觀察,終端狀態)存進replay buffer\n",
    "            observation = observation_ #要在while裡更新新的狀態,不然會一直是舊得observation進行learn\n",
    "            \n",
    "            agent.learn() #迭代中進行TD學習\n",
    "            if action  == 0 :\n",
    "                action0_number += 1\n",
    "            elif action  == 1:\n",
    "                action1_number += 1\n",
    "            elif action  == 2:\n",
    "                action2_number += 1\n",
    "            elif action  == 3:\n",
    "                action3_number += 1\n",
    "        print(\"action_number = \", action0_number,action1_number,action2_number,action3_number)\n",
    "        \n",
    "        \n",
    "        eps_history.append(agent.epsilon)\n",
    "        score = score/(env.data_time_slot)\n",
    "        ddqn_scores.append(score)\n",
    "        avg_score = np.mean(ddqn_scores[-50:])\n",
    "        \n",
    "\n",
    "                \n",
    "        \n",
    "        \n",
    "        if score > best_score : #如果平均分數大於最佳分數\n",
    "            best_score = score #覆蓋掉最佳分數\n",
    "            best_latency_csv=pd.DataFrame(data=csv_latency)\n",
    "            print(\"best_score loading = \",best_score)\n",
    "            best_latency_csv.to_csv('plots/perason_kstest_plot/best_score_latency_60mbps.csv',encoding='utf-8', index = False)\n",
    "            \n",
    "        print('episode: ', i,'score: %.5f' % score,' average score %.5f' % avg_score , 'epsilon %.2f' % agent.epsilon )\n",
    "#         if i>1000: \n",
    "#             if avg_score > -0.62:\n",
    "#                 break_count = break_count + 1\n",
    "#                 print(\"break_count = \",break_count)\n",
    "#         if break_count > 100 and avg_score > -0.62 :\n",
    "#             n_games = i+1\n",
    "#             print(\"n_games = \",n_games)\n",
    "#             break\n",
    "    filename = 'offloading_ddqn_60mbps.png'\n",
    "    end = time.time()\n",
    "    print(\"總共所花的時間\",format(end-start))\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    plotLearning(x, ddqn_scores, eps_history, filename)\n",
    "    for i in range(n_games):\n",
    "        ddqn_scores[i] =-ddqn_scores[i] \n",
    "        \n",
    "    N = len(ddqn_scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(ddqn_scores[max(0, t-50):(t+1)])\n",
    "    print(running_avg)    \n",
    "    test=pd.DataFrame(data=running_avg)\n",
    "    test.to_csv('plots/all_avg_score_offloading_plot/DDQN_avg_score_60mbps.csv',encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 5G 傳輸\n",
    "import numpy as np\n",
    "from dueling_ddqn_tf2 import Agent\n",
    "from utils import plotLearning\n",
    "from enviroment_offloading import offloading_env\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "VtoM_trans = 110  #Mbps以5G傳輸速度為範本\n",
    "VtoV_trans = 165  #Mbps\n",
    "VtoC_trans = 110  #Mbps\n",
    "\n",
    "car_process = 80  #Mbps\n",
    "MEC_process = 170 #Mbps\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    #tf.compat.v1.disable_eager_execution()#關閉tensorflow2裡的eager_execution，eager_execution這會讓tf執行的很慢\n",
    "    #env = gym.make('LunarLander-v2')\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "    env = offloading_env(VtoM_trans,VtoV_trans,VtoC_trans,car_process,MEC_process)\n",
    "    agent = Agent(lr=0.0001, gamma=0.9, n_actions = env.action_space, epsilon=1.0,\n",
    "                  batch_size=64, input_dims=env.observation_space)\n",
    "    n_games = 1500\n",
    "    break_count = 0\n",
    "    ddqn_scores = []\n",
    "    eps_history = []\n",
    "    best_score = -1.6#平均要超過才能保存\n",
    "    print(\"break_count = \",break_count)\n",
    "    for i in range(n_games):\n",
    "        \n",
    "        action0_number = 0\n",
    "        action1_number = 0\n",
    "        action2_number = 0\n",
    "        action3_number = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        setp_number = -1\n",
    "        car1data = 0\n",
    "        car2data = 0\n",
    "        MECdata = 0\n",
    "        score = 0\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            setp_number =setp_number +1\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info,car1data ,car2data ,MECdata  ,csv_latency = env.step(action,setp_number,car1data ,car2data ,MECdata)            \n",
    "#             print(\"第\",setp_number,\"個action = \", action)\n",
    "#             print(\"延遲 = \",reward)\n",
    "#             print(\"第一台車現在的數據輛 = \",observation_[0])\n",
    "#             print(\"第一台車之前的數據輛 = \",car1data)  \n",
    "            \n",
    "#             print(\"第二台車現在的數據輛 = \",observation_[1])\n",
    "#             print(\"第二台車之前的數據輛 = \",car2data)\n",
    "            \n",
    "#             print(\"MEC現在的數據輛 = \",observation_[2])   \n",
    "#             print(\"MEC之前的數據輛 = \",MECdata)\n",
    "#             print(\"========================================\")\n",
    "            score += reward #這次得到的分數就是這一圈epoch的所有reward加總的值,然後迴圈一直更新,直到done為true,就是跑完這次epoch的分數\n",
    "            #score = score - reward\n",
    "#             print(\"score = \",score)\n",
    "            agent.store_transition(observation, action, reward, observation_, done) #把這個迴圈的資料(觀察,動作,獎勵,下一個觀察,終端狀態)存進replay buffer\n",
    "            observation = observation_ #要在while裡更新新的狀態,不然會一直是舊得observation進行learn\n",
    "            \n",
    "            agent.learn() #迭代中進行TD學習\n",
    "            if action  == 0 :\n",
    "                action0_number += 1\n",
    "            elif action  == 1:\n",
    "                action1_number += 1\n",
    "            elif action  == 2:\n",
    "                action2_number += 1\n",
    "            elif action  == 3:\n",
    "                action3_number += 1\n",
    "        print(\"action_number = \", action0_number,action1_number,action2_number,action3_number)\n",
    "        \n",
    "        \n",
    "        eps_history.append(agent.epsilon)\n",
    "        score = score/(env.data_time_slot)\n",
    "        ddqn_scores.append(score)\n",
    "        avg_score = np.mean(ddqn_scores[-50:])\n",
    "        \n",
    "\n",
    "                \n",
    "        \n",
    "        \n",
    "        if score > best_score : #如果平均分數大於最佳分數\n",
    "            best_score = score #覆蓋掉最佳分數\n",
    "            best_latency_csv=pd.DataFrame(data=csv_latency)\n",
    "            print(\"best_score loading = \",best_score)\n",
    "            best_latency_csv.to_csv('plots/perason_kstest_plot/best_score_latency_80mbps.csv',encoding='utf-8', index = False)\n",
    "            \n",
    "        print('episode: ', i,'score: %.5f' % score,' average score %.5f' % avg_score , 'epsilon %.2f' % agent.epsilon )\n",
    "#         if i>1000: \n",
    "#             if avg_score > -0.62:\n",
    "#                 break_count = break_count + 1\n",
    "#                 print(\"break_count = \",break_count)\n",
    "#         if break_count > 100 and avg_score > -0.62 :\n",
    "#             n_games = i+1\n",
    "#             print(\"n_games = \",n_games)\n",
    "#             break\n",
    "    filename = 'offloading_ddqn_0.8.png'\n",
    "    end = time.time()\n",
    "    print(\"總共所花的時間\",format(end-start))\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    plotLearning(x, ddqn_scores, eps_history, filename)\n",
    "    for i in range(n_games):\n",
    "        ddqn_scores[i] =-ddqn_scores[i] \n",
    "        \n",
    "    N = len(ddqn_scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(ddqn_scores[max(0, t-50):(t+1)])\n",
    "    print(running_avg)    \n",
    "    test=pd.DataFrame(data=running_avg)\n",
    "    test.to_csv('plots/all_avg_score_offloading_plot/best_score_latency_80mbps.csv',encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 5G 傳輸\n",
    "import numpy as np\n",
    "from dueling_ddqn_tf2 import Agent\n",
    "from utils import plotLearning\n",
    "from enviroment_offloading import offloading_env\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "VtoM_trans = 110  #Mbps以5G傳輸速度為範本\n",
    "VtoV_trans = 165  #Mbps\n",
    "VtoC_trans = 110  #Mbps\n",
    "\n",
    "car_process = 120  #Mbps\n",
    "MEC_process = 170 #Mbps\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    #tf.compat.v1.disable_eager_execution()#關閉tensorflow2裡的eager_execution，eager_execution這會讓tf執行的很慢\n",
    "    #env = gym.make('LunarLander-v2')\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "    env = offloading_env(VtoM_trans,VtoV_trans,VtoC_trans,car_process,MEC_process)\n",
    "    agent = Agent(lr=0.0001, gamma=0.9, n_actions = env.action_space, epsilon=1.0,\n",
    "                  batch_size=64, input_dims=env.observation_space)\n",
    "    n_games = 1500\n",
    "    break_count = 0\n",
    "    ddqn_scores = []\n",
    "    eps_history = []\n",
    "    best_score = -1.6#平均要超過才能保存\n",
    "    print(\"break_count = \",break_count)\n",
    "    for i in range(n_games):\n",
    "        \n",
    "        action0_number = 0\n",
    "        action1_number = 0\n",
    "        action2_number = 0\n",
    "        action3_number = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        setp_number = -1\n",
    "        car1data = 0\n",
    "        car2data = 0\n",
    "        MECdata = 0\n",
    "        score = 0\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            setp_number =setp_number +1\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info,car1data ,car2data ,MECdata  ,csv_latency = env.step(action,setp_number,car1data ,car2data ,MECdata)            \n",
    "#             print(\"第\",setp_number,\"個action = \", action)\n",
    "#             print(\"延遲 = \",reward)\n",
    "#             print(\"第一台車現在的數據輛 = \",observation_[0])\n",
    "#             print(\"第一台車之前的數據輛 = \",car1data)  \n",
    "            \n",
    "#             print(\"第二台車現在的數據輛 = \",observation_[1])\n",
    "#             print(\"第二台車之前的數據輛 = \",car2data)\n",
    "            \n",
    "#             print(\"MEC現在的數據輛 = \",observation_[2])   \n",
    "#             print(\"MEC之前的數據輛 = \",MECdata)\n",
    "#             print(\"========================================\")\n",
    "            score += reward #這次得到的分數就是這一圈epoch的所有reward加總的值,然後迴圈一直更新,直到done為true,就是跑完這次epoch的分數\n",
    "            #score = score - reward\n",
    "#             print(\"score = \",score)\n",
    "            agent.store_transition(observation, action, reward, observation_, done) #把這個迴圈的資料(觀察,動作,獎勵,下一個觀察,終端狀態)存進replay buffer\n",
    "            observation = observation_ #要在while裡更新新的狀態,不然會一直是舊得observation進行learn\n",
    "            \n",
    "            agent.learn() #迭代中進行TD學習\n",
    "            if action  == 0 :\n",
    "                action0_number += 1\n",
    "            elif action  == 1:\n",
    "                action1_number += 1\n",
    "            elif action  == 2:\n",
    "                action2_number += 1\n",
    "            elif action  == 3:\n",
    "                action3_number += 1\n",
    "        print(\"action_number = \", action0_number,action1_number,action2_number,action3_number)\n",
    "        \n",
    "        \n",
    "        eps_history.append(agent.epsilon)\n",
    "        score = score/(env.data_time_slot)\n",
    "        ddqn_scores.append(score)\n",
    "        avg_score = np.mean(ddqn_scores[-50:])\n",
    "        \n",
    "\n",
    "                \n",
    "        \n",
    "        \n",
    "        if score > best_score : #如果平均分數大於最佳分數\n",
    "            best_score = score #覆蓋掉最佳分數\n",
    "            best_latency_csv=pd.DataFrame(data=csv_latency)\n",
    "            print(\"best_score loading = \",best_score)\n",
    "            best_latency_csv.to_csv('plots/perason_kstest_plot/best_score_latency_120mbps.csv',encoding='utf-8', index = False)\n",
    "            \n",
    "        print('episode: ', i,'score: %.5f' % score,' average score %.5f' % avg_score , 'epsilon %.2f' % agent.epsilon )\n",
    "#         if i>1000: \n",
    "#             if avg_score > -0.62:\n",
    "#                 break_count = break_count + 1\n",
    "#                 print(\"break_count = \",break_count)\n",
    "#         if break_count > 100 and avg_score > -0.62 :\n",
    "#             n_games = i+1\n",
    "#             print(\"n_games = \",n_games)\n",
    "#             break\n",
    "    filename = 'offloading_ddqn_0.85.png'\n",
    "    end = time.time()\n",
    "    print(\"總共所花的時間\",format(end-start))\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    plotLearning(x, ddqn_scores, eps_history, filename)\n",
    "    for i in range(n_games):\n",
    "        ddqn_scores[i] =-ddqn_scores[i] \n",
    "        \n",
    "    N = len(ddqn_scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(ddqn_scores[max(0, t-50):(t+1)])\n",
    "    print(running_avg)    \n",
    "    test=pd.DataFrame(data=running_avg)\n",
    "    test.to_csv('plots/all_avg_score_offloading_plot/best_score_latency_120mbps.csv',encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 5G 傳輸\n",
    "import numpy as np\n",
    "from dueling_ddqn_tf2 import Agent\n",
    "from utils import plotLearning\n",
    "from enviroment_offloading import offloading_env\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "VtoM_trans = 110  #Mbps以5G傳輸速度為範本\n",
    "VtoV_trans = 165  #Mbps\n",
    "VtoC_trans = 110  #Mbps\n",
    "\n",
    "car_process = 100  #Mbps\n",
    "MEC_process = 170 #Mbps\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    #tf.compat.v1.disable_eager_execution()#關閉tensorflow2裡的eager_execution，eager_execution這會讓tf執行的很慢\n",
    "    #env = gym.make('LunarLander-v2')\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "    env = offloading_env(VtoM_trans,VtoV_trans,VtoC_trans,car_process,MEC_process)\n",
    "    agent = Agent(lr=0.0001, gamma=0.9, n_actions = env.action_space, epsilon=1.0,\n",
    "                  batch_size=64, input_dims=env.observation_space)\n",
    "    n_games = 1500\n",
    "    break_count = 0\n",
    "    ddqn_scores = []\n",
    "    eps_history = []\n",
    "    best_score = -1.6#平均要超過才能保存\n",
    "    print(\"break_count = \",break_count)\n",
    "    for i in range(n_games):\n",
    "        \n",
    "        action0_number = 0\n",
    "        action1_number = 0\n",
    "        action2_number = 0\n",
    "        action3_number = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        setp_number = -1\n",
    "        car1data = 0\n",
    "        car2data = 0\n",
    "        MECdata = 0\n",
    "        score = 0\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            setp_number =setp_number +1\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info,car1data ,car2data ,MECdata  ,csv_latency = env.step(action,setp_number,car1data ,car2data ,MECdata)            \n",
    "#             print(\"第\",setp_number,\"個action = \", action)\n",
    "#             print(\"延遲 = \",reward)\n",
    "#             print(\"第一台車現在的數據輛 = \",observation_[0])\n",
    "#             print(\"第一台車之前的數據輛 = \",car1data)  \n",
    "            \n",
    "#             print(\"第二台車現在的數據輛 = \",observation_[1])\n",
    "#             print(\"第二台車之前的數據輛 = \",car2data)\n",
    "            \n",
    "#             print(\"MEC現在的數據輛 = \",observation_[2])   \n",
    "#             print(\"MEC之前的數據輛 = \",MECdata)\n",
    "#             print(\"========================================\")\n",
    "            score += reward #這次得到的分數就是這一圈epoch的所有reward加總的值,然後迴圈一直更新,直到done為true,就是跑完這次epoch的分數\n",
    "            #score = score - reward\n",
    "#             print(\"score = \",score)\n",
    "            agent.store_transition(observation, action, reward, observation_, done) #把這個迴圈的資料(觀察,動作,獎勵,下一個觀察,終端狀態)存進replay buffer\n",
    "            observation = observation_ #要在while裡更新新的狀態,不然會一直是舊得observation進行learn\n",
    "            \n",
    "            agent.learn() #迭代中進行TD學習\n",
    "            if action  == 0 :\n",
    "                action0_number += 1\n",
    "            elif action  == 1:\n",
    "                action1_number += 1\n",
    "            elif action  == 2:\n",
    "                action2_number += 1\n",
    "            elif action  == 3:\n",
    "                action3_number += 1\n",
    "        print(\"action_number = \", action0_number,action1_number,action2_number,action3_number)\n",
    "        \n",
    "        \n",
    "        eps_history.append(agent.epsilon)\n",
    "        score = score/(env.data_time_slot)\n",
    "        ddqn_scores.append(score)\n",
    "        avg_score = np.mean(ddqn_scores[-50:])\n",
    "        \n",
    "\n",
    "                \n",
    "        \n",
    "        \n",
    "        if score > best_score : #如果平均分數大於最佳分數\n",
    "            best_score = score #覆蓋掉最佳分數\n",
    "            best_latency_csv=pd.DataFrame(data=csv_latency)\n",
    "            print(\"best_score loading = \",best_score)\n",
    "            best_latency_csv.to_csv('plots/perason_kstest_plot/best_score_latency_100mbps.csv',encoding='utf-8', index = False)\n",
    "            \n",
    "        print('episode: ', i,'score: %.5f' % score,' average score %.5f' % avg_score , 'epsilon %.2f' % agent.epsilon )\n",
    "#         if i>1000: \n",
    "#             if avg_score > -0.62:\n",
    "#                 break_count = break_count + 1\n",
    "#                 print(\"break_count = \",break_count)\n",
    "#         if break_count > 100 and avg_score > -0.62 :\n",
    "#             n_games = i+1\n",
    "#             print(\"n_games = \",n_games)\n",
    "#             break\n",
    "    filename = 'offloading_ddqn_0.8.png'\n",
    "    end = time.time()\n",
    "    print(\"總共所花的時間\",format(end-start))\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    plotLearning(x, ddqn_scores, eps_history, filename)\n",
    "    for i in range(n_games):\n",
    "        ddqn_scores[i] =-ddqn_scores[i] \n",
    "        \n",
    "    N = len(ddqn_scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(ddqn_scores[max(0, t-50):(t+1)])\n",
    "    print(running_avg)    \n",
    "    test=pd.DataFrame(data=running_avg)\n",
    "    test.to_csv('plots/all_avg_score_offloading_plot/best_score_latency_100mbps.csv',encoding='utf-8', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
